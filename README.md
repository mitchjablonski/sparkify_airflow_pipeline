# Sparkify Pipeline Implemented with Airflow
The sparkify music platform has been seeing a great deal of success.  As a result - beyond the scaling of the data base that has been required, there has also been a necessity to automate the running of the pipeline.  The product we chose for implementing this was Apache Airflow due to it being a known tool with a robust ecosystem that has been developed on extensively.  Apache airflow allows us to define a schedule at which we run the tool, implement SLAs for high priority data sets such that we dont miss deadlines, for high priority dat sets.  We can also define the order of operations for our tasks, and easily identify the dependencies in our pipeline with the use of DAGs.


## The Pipeline is managed through our core dag file
The pipeline works by, coping our event data, and song data, from our S3 bucket into our redshift cluster, into their staging tables.  The rest of the pipeline is dependent on these events.  Once the data is staged into redshift, we can proceed.  From there, we are able to generate the songplays table - if both the song and event tables are finished.  We have 3 other tables that could be populated with only 1 dependency though - user, song, and artist, being dependent on events for user, and song for song and aritst.  These 3 tables fit very nicely with a sub dag -following exactly the same steps and depencencies, creating the tables, pushing the data into the tables, and performing a data check.  Our time table is dependent on song plays - so we will wait for that file to be created and push them in, once its met that dependency though - it also meets the subdag requirements, so we will utilize it here as well.  The songplays will also kick off a quality check after completed.  When all the stages wrap up - we can move to ending the execution.